{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/05/07 22:01:33 INFO SparkContext: Running Spark version 3.1.2\n",
      "22/05/07 22:01:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/07 22:01:34 INFO ResourceUtils: ==============================================================\n",
      "22/05/07 22:01:34 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/05/07 22:01:34 INFO ResourceUtils: ==============================================================\n",
      "22/05/07 22:01:34 INFO SparkContext: Submitted application: ad38aadd-a33e-47a6-a0d7-46c474630ce7\n",
      "22/05/07 22:01:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/05/07 22:01:34 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/05/07 22:01:34 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/05/07 22:01:34 INFO SecurityManager: Changing view acls to: jovyan\n",
      "22/05/07 22:01:34 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "22/05/07 22:01:34 INFO SecurityManager: Changing view acls groups to: \n",
      "22/05/07 22:01:34 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/05/07 22:01:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "22/05/07 22:01:34 INFO Utils: Successfully started service 'sparkDriver' on port 34307.\n",
      "22/05/07 22:01:34 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/05/07 22:01:34 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/05/07 22:01:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/05/07 22:01:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/05/07 22:01:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/07 22:01:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fa98bf0a-de94-4c6c-9368-0f58af2eb97a\n",
      "22/05/07 22:01:34 INFO MemoryStore: MemoryStore started with capacity 1866.0 MiB\n",
      "22/05/07 22:01:34 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/05/07 22:01:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/05/07 22:01:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://280f92fa5b7d:4040\n",
      "22/05/07 22:01:35 INFO Executor: Starting executor ID driver on host 280f92fa5b7d\n",
      "22/05/07 22:01:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40039.\n",
      "22/05/07 22:01:35 INFO NettyBlockTransferService: Server created on 280f92fa5b7d:40039\n",
      "22/05/07 22:01:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/05/07 22:01:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 280f92fa5b7d, 40039, None)\n",
      "22/05/07 22:01:35 INFO BlockManagerMasterEndpoint: Registering block manager 280f92fa5b7d:40039 with 1866.0 MiB RAM, BlockManagerId(driver, 280f92fa5b7d, 40039, None)\n",
      "22/05/07 22:01:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 280f92fa5b7d, 40039, None)\n",
      "22/05/07 22:01:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 280f92fa5b7d, 40039, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\n",
       "// Load spark-sql usual toolbox\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4792a39\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load spark library using ivy (dependendcy manager)\n",
    "import $ivy.`org.apache.spark::spark-sql:3.1.2`\n",
    "\n",
    "// Load spark-sql usual toolbox\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = {\n",
    "    var r = SparkSession.builder.master(\"local[*]\").getOrCreate\n",
    "    r.sparkContext.setLogLevel(\"ERROR\")\n",
    "    r\n",
    "}\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mseed\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2022L\u001b[39m\n",
       "\u001b[36mfrom_unixdays\u001b[39m: \u001b[32mColumn\u001b[39m => \u001b[32mColumn\u001b[39m = ammonite.$sess.cmd1$Helper$$Lambda$3430/1854194172@413009a5\n",
       "\u001b[36mswap\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint, date: string ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 2022L\n",
    "val from_unixdays = (n: Column) => from_unixtime(n *  86400)\n",
    "val swap = spark.range(3)\n",
    "    .withColumn(\"date\", from_unixdays($\"id\"))\n",
    "    .withColumn(\"points\", lit(1000) + round(lit(100)*rand(seed), 0).cast(IntegerType) * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+\n",
      "| id|               date|points|\n",
      "+---+-------------------+------+\n",
      "|  0|1970-01-01 00:00:00|  1090|\n",
      "|  1|1970-01-02 00:00:00|  1730|\n",
      "|  2|1970-01-03 00:00:00|  1400|\n",
      "+---+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swap.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
